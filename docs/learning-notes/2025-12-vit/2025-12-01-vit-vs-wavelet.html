<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.30">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="ChatGPT-5.1 Thinking, Yulin Li">
<meta name="dcterms.date" content="2025-12-01">

<title>ViT vs.&nbsp;Wavelet Intuition</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="2025-12-01-vit-vs-wavelet_files/libs/clipboard/clipboard.min.js"></script>
<script src="2025-12-01-vit-vs-wavelet_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="2025-12-01-vit-vs-wavelet_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="2025-12-01-vit-vs-wavelet_files/libs/quarto-html/popper.min.js"></script>
<script src="2025-12-01-vit-vs-wavelet_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="2025-12-01-vit-vs-wavelet_files/libs/quarto-html/anchor.min.js"></script>
<link href="2025-12-01-vit-vs-wavelet_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="2025-12-01-vit-vs-wavelet_files/libs/quarto-html/quarto-syntax-highlighting-de070a7b0ab54f8780927367ac907214.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="2025-12-01-vit-vs-wavelet_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="2025-12-01-vit-vs-wavelet_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="2025-12-01-vit-vs-wavelet_files/libs/bootstrap/bootstrap-81267100e462c21b3d6c0d5bf76a3417.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#background-what-is-vit" id="toc-background-what-is-vit" class="nav-link active" data-scroll-target="#background-what-is-vit"><span class="header-section-number">1</span> Background: What is ViT?</a></li>
  <li><a href="#vit-adaptivity-to-scale" id="toc-vit-adaptivity-to-scale" class="nav-link" data-scroll-target="#vit-adaptivity-to-scale"><span class="header-section-number">2</span> ViT: Adaptivity to Scale</a>
  <ul class="collapse">
  <li><a href="#how-does-it-decide-the-scale-to-look-at" id="toc-how-does-it-decide-the-scale-to-look-at" class="nav-link" data-scroll-target="#how-does-it-decide-the-scale-to-look-at"><span class="header-section-number">2.1</span> How does it decide the scale to look at?</a></li>
  <li><a href="#how-do-you-know-if-your-visual-vocab-is-expressive-enough" id="toc-how-do-you-know-if-your-visual-vocab-is-expressive-enough" class="nav-link" data-scroll-target="#how-do-you-know-if-your-visual-vocab-is-expressive-enough"><span class="header-section-number">2.2</span> How do you know if your visual vocab is expressive enough?</a></li>
  <li><a href="#what-about-multi-resolution-wavelet-like-behavior" id="toc-what-about-multi-resolution-wavelet-like-behavior" class="nav-link" data-scroll-target="#what-about-multi-resolution-wavelet-like-behavior"><span class="header-section-number">2.3</span> What about multi-resolution / wavelet-like behavior?</a></li>
  <li><a href="#how-this-ties-back-to-the-llm-analogy" id="toc-how-this-ties-back-to-the-llm-analogy" class="nav-link" data-scroll-target="#how-this-ties-back-to-the-llm-analogy"><span class="header-section-number">2.4</span> How this ties back to the ‚ÄúLLM analogy‚Äù</a></li>
  </ul></li>
  <li><a href="#how-attention-grows-with-depth" id="toc-how-attention-grows-with-depth" class="nav-link" data-scroll-target="#how-attention-grows-with-depth"><span class="header-section-number">3</span> How Attention Grows with Depth</a>
  <ul class="collapse">
  <li><a href="#stacking-transformer-blocks" id="toc-stacking-transformer-blocks" class="nav-link" data-scroll-target="#stacking-transformer-blocks"><span class="header-section-number">3.1</span> Stacking Transformer Blocks</a></li>
  <li><a href="#how-does-that-make-attention-grow" id="toc-how-does-that-make-attention-grow" class="nav-link" data-scroll-target="#how-does-that-make-attention-grow"><span class="header-section-number">3.2</span> How does that make attention ‚Äúgrow‚Äù?</a></li>
  <li><a href="#core-intuition" id="toc-core-intuition" class="nav-link" data-scroll-target="#core-intuition"><span class="header-section-number">3.3</span> Core Intuition</a></li>
  </ul></li>
  <li><a href="#vit-recognition-and-generation" id="toc-vit-recognition-and-generation" class="nav-link" data-scroll-target="#vit-recognition-and-generation"><span class="header-section-number">4</span> ViT: Recognition and Generation</a>
  <ul class="collapse">
  <li><a href="#recognitionperception" id="toc-recognitionperception" class="nav-link" data-scroll-target="#recognitionperception"><span class="header-section-number">4.1</span> Recognition/Perception</a></li>
  <li><a href="#generation" id="toc-generation" class="nav-link" data-scroll-target="#generation"><span class="header-section-number">4.2</span> Generation</a></li>
  </ul></li>
  <li><a href="#vit-efficiency-of-representation" id="toc-vit-efficiency-of-representation" class="nav-link" data-scroll-target="#vit-efficiency-of-representation"><span class="header-section-number">5</span> ViT: Efficiency of Representation üîç</a>
  <ul class="collapse">
  <li><a href="#vanilla-vit-is-not-shift-equivariant" id="toc-vanilla-vit-is-not-shift-equivariant" class="nav-link" data-scroll-target="#vanilla-vit-is-not-shift-equivariant"><span class="header-section-number">5.1</span> Vanilla ViT is Not Shift Equivariant</a></li>
  <li><a href="#but-its-not-combinatorially-non-smooth-in-the-math-sense" id="toc-but-its-not-combinatorially-non-smooth-in-the-math-sense" class="nav-link" data-scroll-target="#but-its-not-combinatorially-non-smooth-in-the-math-sense"><span class="header-section-number">5.2</span> But it‚Äôs not ‚Äúcombinatorially non-smooth‚Äù in the math sense</a></li>
  <li><a href="#how-vits-actually-get-approximate-invariance" id="toc-how-vits-actually-get-approximate-invariance" class="nav-link" data-scroll-target="#how-vits-actually-get-approximate-invariance"><span class="header-section-number">5.3</span> How ViTs <em>actually</em> get approximate invariance</a></li>
  <li><a href="#multi-resolution-gets-added-back" id="toc-multi-resolution-gets-added-back" class="nav-link" data-scroll-target="#multi-resolution-gets-added-back"><span class="header-section-number">5.4</span> Multi-Resolution Gets Added Back</a></li>
  <li><a href="#tldr" id="toc-tldr" class="nav-link" data-scroll-target="#tldr"><span class="header-section-number">5.5</span> TL;DR</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">ViT vs.&nbsp;Wavelet Intuition</h1>
<p class="subtitle lead">Patch scale, multiresolution, and shift robustness in ViT backbones</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>ChatGPT-5.1 Thinking, Yulin Li </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">December 1, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="background-what-is-vit" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="background-what-is-vit"><span class="header-section-number">1</span> Background: What is ViT?</h2>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<p><em>What is CLIP in LLM? What is ViT? One-line definitions plz.</em></p>
</div>
</div>
<dl>
<dt><strong>CLIP (Contrastive Language‚ÄìImage Pre-training)</strong></dt>
<dd>
<p>A vision-language model that jointly trains an image encoder and a text encoder so their embeddings live in the same space, letting models ‚Äúmatch‚Äù pictures to natural-language descriptions.</p>
</dd>
<dt><strong>ViT (Vision Transformer)</strong></dt>
<dd>
<p>A vision backbone that treats an image as a sequence of patches and runs a Transformer over them, analogous to how LLMs process token sequences.</p>
</dd>
</dl>
<hr>
</section>
<section id="vit-adaptivity-to-scale" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="vit-adaptivity-to-scale"><span class="header-section-number">2</span> ViT: Adaptivity to Scale</h2>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Questions
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><em>How does ViT decide the scale to look at?</em></li>
<li><em>How do you know if your visual vocab is expressive enough for the whole space of natural images?</em></li>
<li><em>Usually for image processing we‚Äôd need to look at a picture at different scales‚Äîlike Haar wavelets. Was multi-resolution decomposition ever involved in ViT-based architectures?</em></li>
</ul>
</div>
</div>
<p>This is exactly the right thing to worry about with ‚Äúimage as tokens.‚Äù Let me break this down:</p>
<section id="how-does-it-decide-the-scale-to-look-at" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="how-does-it-decide-the-scale-to-look-at"><span class="header-section-number">2.1</span> How does it decide the scale to look at?</h3>
<p>In a plain ViT (like the original Dosovitskiy et al.):</p>
<ul>
<li>You <strong>pick a fixed patch size</strong> (e.g.&nbsp;16√ó16).</li>
<li>The image is split into non-overlapping patches of that size.</li>
<li>Each patch ‚Üí linearly projected ‚Üí one token.</li>
<li>A Transformer with <strong>global self-attention</strong> lets every patch talk to every other patch.</li>
</ul>
<p>So:</p>
<ul>
<li><strong>Local detail</strong> comes from the fact that a token only ‚Äúsees‚Äù its own 16√ó16 pixels before attention.</li>
<li><strong>Global structure</strong> comes from self-attention across <em>all</em> tokens, which lets the model integrate info over the full image.</li>
</ul>
<p>There‚Äôs no explicit ‚Äúpyramid of scales‚Äù like in classical wavelets or CNN feature pyramids. The <em>scale</em> is implicitly controlled by:</p>
<ol type="1">
<li><strong>Patch size</strong> (how local each token is), and</li>
<li><strong>Depth of the Transformer</strong> (how many layers of attention you get to mix local‚Üíglobal).</li>
</ol>
<p>In other words: vanilla ViT has <strong>one native resolution</strong> (fixed patch size), but attention is global, so the <em>effective</em> receptive field of each token grows as you go deeper.</p>
</section>
<section id="how-do-you-know-if-your-visual-vocab-is-expressive-enough" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="how-do-you-know-if-your-visual-vocab-is-expressive-enough"><span class="header-section-number">2.2</span> How do you know if your visual vocab is expressive enough?</h3>
<p>Here ‚Äúvisual vocab‚Äù ‚âà the space of patch embeddings / learned features.</p>
<p>For ViTs:</p>
<ul>
<li>There is <strong>no fixed, pre-defined dictionary</strong> like SIFT/Haar.</li>
<li>Instead, the model learns an embedding function <code>f(patch)</code> from data, exactly like a word-embedding layer learns meanings from corpus statistics.</li>
<li>If the model is trained on a <strong>huge, diverse dataset</strong> (e.g.&nbsp;ImageNet-21k, JFT, LAION) with a strong objective (supervised or contrastive), the learned patch-embedding space tends to become expressive enough in practice.</li>
</ul>
<p>So how do we <em>actually</em> know?</p>
<ul>
<li><p>Empirically: if a ViT backbone works well across <strong>many downstream tasks</strong> (classification, detection, segmentation, retrieval) with moderate fine-tuning, that‚Äôs evidence your visual ‚Äúvocab‚Äù is rich enough.</p></li>
<li><p>Theoretically, there‚Äôs no guarantee it perfectly spans ‚Äúthe space of natural images,‚Äù same as we don‚Äôt guarantee LLM token embeddings span ‚Äúall possible meanings.‚Äù It‚Äôs an approximation that improves with:</p>
<ul>
<li>model capacity (width/depth),</li>
<li>better objectives (contrastive, masked prediction),</li>
<li>and more diverse data.</li>
</ul></li>
</ul>
<p>CLIP is a nice example: it <strong>anchors visual embeddings to language</strong>. If your text side can name ‚Äútuxedo cat,‚Äù ‚ÄúMRI slice,‚Äù ‚Äúanime screencap,‚Äù etc., then the image encoder is pressured to carve up visual space in a way that aligns with those concepts ‚Äî a kind of <em>semantic</em> visual vocabulary.</p>
</section>
<section id="what-about-multi-resolution-wavelet-like-behavior" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="what-about-multi-resolution-wavelet-like-behavior"><span class="header-section-number">2.3</span> What about multi-resolution / wavelet-like behavior?</h3>
<p>Vanilla ViT:</p>
<ul>
<li><strong>No explicit multiscale decomposition</strong> like Haar wavelets or image pyramids.</li>
<li>It‚Äôs single-scale patches + global attention.</li>
</ul>
<p>But the ViT <em>ecosystem</em> quickly introduced <strong>hierarchical / multiscale variants</strong> precisely because tasks like detection/segmentation <em>do</em> benefit from multi-resolution features (like CNNs):</p>
<ul>
<li><strong>Swin Transformer</strong>: computes self-attention in local windows and <strong>merges patches</strong> between stages ‚Üí a <strong>feature pyramid</strong> (high-res shallow layers, low-res deep layers).</li>
<li><strong>PVT (Pyramid Vision Transformer)</strong>, <strong>MViT (Multiscale ViT)</strong>, etc.: explicitly build <strong>hierarchical, multi-resolution feature maps</strong>, much closer in flavor to FPN/ResNet pyramids.</li>
<li>Many modern ViT-based backbones for detection/segmentation give you outputs at multiple strides (e.g., 1/4, 1/8, 1/16 of the input), like a CNN.</li>
</ul>
<p>So:</p>
<ul>
<li><strong>Original ViT</strong>: one-scale patches, global attention; multiscale behavior is <em>implicit</em> via depth and attention patterns, not a clean wavelet-like decomposition.</li>
<li><strong>Modern ViT-based backbones</strong>: <em>do</em> incorporate multi-resolution structures (patch merging, pyramids, windowed attention) so they can act more like classical multiscale vision systems.</li>
</ul>
</section>
<section id="how-this-ties-back-to-the-llm-analogy" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="how-this-ties-back-to-the-llm-analogy"><span class="header-section-number">2.4</span> How this ties back to the ‚ÄúLLM analogy‚Äù</h3>
<p>When I said:</p>
<blockquote class="blockquote">
<p>‚Äútreats an image as a sequence of patches and runs a Transformer over them, analogous to how LLMs process token sequences‚Äù</p>
</blockquote>
<p>more precisely it means:</p>
<ul>
<li>Image patches ‚âà text tokens.</li>
<li>Patch embedding layer ‚âà token embedding layer.</li>
<li>Self-attention layers operate over <strong>the sequence</strong>, not over a 2D grid with fixed kernels.</li>
<li>In vanilla ViT, there‚Äôs <strong>one token granularity</strong> (patch size), just like LLMs don‚Äôt usually change token length at deeper layers.</li>
</ul>
<p>Multiscale ViT variants correspond, roughly, to <strong>multi-resolution representations</strong> (like CNN pyramids or wavelets), layered on top of this ‚Äúimage-as-sequence‚Äù idea.</p>
<hr>
<p>If you‚Äôd like, next step we can:</p>
<ul>
<li>Compare one concrete architecture: plain ViT vs.&nbsp;Swin, and</li>
<li>Map it explicitly to wavelet intuition (what in Swin/PVT plays the role of ‚Äúdownsampling‚Äù and ‚Äúlarger-scale‚Äù basis functions).</li>
</ul>
<hr>
</section>
</section>
<section id="how-attention-grows-with-depth" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="how-attention-grows-with-depth"><span class="header-section-number">3</span> How Attention Grows with Depth</h2>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<p><em>How does attention grow in depth? Do you stack Transformers somehow?</em></p>
</div>
</div>
<p>Yes‚Äîyou stack Transformer blocks, and that‚Äôs how ‚Äúmixing over larger scales‚Äù happens.</p>
<hr>
<section id="stacking-transformer-blocks" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="stacking-transformer-blocks"><span class="header-section-number">3.1</span> Stacking Transformer Blocks</h3>
<p>A ViT (and an LLM) has this structure:</p>
<pre class="text"><code>tokens ‚Üí [Block 1] ‚Üí [Block 2] ‚Üí ... ‚Üí [Block L] ‚Üí head</code></pre>
<p>Each <strong>Block</strong> consists of:</p>
<ul>
<li>LayerNorm</li>
<li>Multi-Head Self-Attention</li>
<li>MLP</li>
<li>Residual connections</li>
</ul>
<p>All blocks have the <strong>same structure</strong>, but their weights are different and learned jointly. You‚Äôre literally stacking self-attention layers, one on top of another.</p>
<hr>
</section>
<section id="how-does-that-make-attention-grow" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="how-does-that-make-attention-grow"><span class="header-section-number">3.2</span> How does that make attention ‚Äúgrow‚Äù?</h3>
<p>Two cases: <strong>global attention</strong> (plain ViT) vs <strong>local/windowed attention</strong> (e.g., Swin).</p>
<section id="vanilla-vit-global-self-attention" class="level4" data-number="3.2.1">
<h4 data-number="3.2.1" class="anchored" data-anchor-id="vanilla-vit-global-self-attention"><span class="header-section-number">3.2.1</span> Vanilla ViT (global self-attention)</h4>
<ul>
<li>In <em>one</em> layer, each token can already attend to <strong>all</strong> other tokens.</li>
<li>So in a strict graph-theory sense, the receptive field is already ‚Äúglobal‚Äù in one hop.</li>
</ul>
<p>So why does depth still matter?</p>
<ul>
<li><p>Early layers tend to encode <strong>local, low-level stuff</strong> (edges, textures within a patch and near neighbors).</p></li>
<li><p>Later layers operate on <strong>already-processed features</strong>, so they can:</p>
<ul>
<li>attend to <em>summaries</em> (e.g., ‚Äúthis patch already encodes ‚Äòcat ear‚Äô or ‚Äòcolumn of text‚Äô‚Äù),</li>
<li>combine those into more global concepts (‚Äúwhole cat,‚Äù ‚Äúentire title line‚Äù),</li>
<li>refine and reweight interactions iteratively (like multiple rounds of message passing).</li>
</ul></li>
</ul>
<p>So depth ‚âà <em>more rounds of ‚Äúwho should I pay attention to, given what I‚Äôve learned so far?‚Äù</em>. Even though every layer can see the whole sequence, what they <em>do</em> with that view becomes more global and abstract as you go deeper.</p>
<hr>
</section>
<section id="windowedlocal-attention-swin-etc." class="level4" data-number="3.2.2">
<h4 data-number="3.2.2" class="anchored" data-anchor-id="windowedlocal-attention-swin-etc."><span class="header-section-number">3.2.2</span> Windowed/Local Attention (Swin, etc.)</h4>
<p>Here it‚Äôs more literally ‚Äúgrowing receptive field‚Äù:</p>
<ol type="1">
<li><strong>Layer 1</strong>: Each token attends only within a small window (e.g., 7√ó7 patches)</li>
<li><strong>Layer 2</strong>: Windows shift / patches merge ‚Üí each token now indirectly depends on a <strong>bigger neighborhood</strong></li>
<li><strong>After several layers</strong>: The ‚Äúcommunication radius‚Äù expands; tokens end up influenced by faraway regions</li>
</ol>
<p>This is the Transformer analogue of: <strong>more conv layers ‚Üí larger effective receptive field</strong>.</p>
<hr>
</section>
</section>
<section id="core-intuition" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="core-intuition"><span class="header-section-number">3.3</span> Core Intuition</h3>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>We stack many Transformer blocks; each self-attention layer lets tokens re-mix information based on what they‚Äôve already computed. As you go deeper, tokens‚Äô representations fold in more context and more abstract structure‚Äîeven if each layer is ‚Äújust‚Äù the same attention mechanism repeated.</p>
</div>
</div>
</div>
<hr>
</section>
</section>
<section id="vit-recognition-and-generation" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="vit-recognition-and-generation"><span class="header-section-number">4</span> ViT: Recognition and Generation</h2>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<p><em>Is ViT used only for recognition? Or generation as well?</em></p>
</div>
</div>
<p>Definitely <strong>not</strong> only for recognition‚ÄîViT is used for <strong>both</strong>.</p>
<section id="recognitionperception" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="recognitionperception"><span class="header-section-number">4.1</span> Recognition/Perception</h3>
<p>The original ViT was for image classification. ViT-style backbones are now everywhere:</p>
<ul>
<li>Image classification</li>
<li>Object detection</li>
<li>Semantic segmentation</li>
<li>Image retrieval</li>
<li>CLIP-style encoders</li>
</ul>
</section>
<section id="generation" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="generation"><span class="header-section-number">4.2</span> Generation</h3>
<p>ViTs are used in two ways:</p>
<dl>
<dt><strong>As encoders feeding generators</strong></dt>
<dd>
<p>Examples: ViT ‚Üí LLM ‚Üí text, or ViT ‚Üí diffusion model ‚Üí edited image</p>
</dd>
<dt><strong>As image generators themselves</strong></dt>
<dd>
<p>When you treat image patches as tokens and train a Transformer to <em>predict or denoise</em> them:</p>
</dd>
</dl>
<ul>
<li>Masked image modeling</li>
<li>Autoregressive image token models</li>
<li>Diffusion Transformers (‚ÄúDiT‚Äù-style models)</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Key takeaway:</strong> ViT = a general-purpose vision backbone. Whether it ‚Äúrecognizes‚Äù or ‚Äúgenerates‚Äù depends on the head + training objective, not the architecture itself.</p>
</div>
</div>
<hr>
</section>
</section>
<section id="vit-efficiency-of-representation" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="vit-efficiency-of-representation"><span class="header-section-number">5</span> ViT: Efficiency of Representation üîç</h2>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<p><em>The naive ViT idea seems very weird. Without multi-resolution adaptability, the function from image to visual tokens/image embedding looks very non-smooth. If you shift the image by 1 column of pixels to the right, then the whole message flow changes‚Äîunless you heavily overparametrize‚Ä¶?</em></p>
</div>
</div>
<p>This intuition is on point: <strong>a strictly ‚Äúnaive‚Äù ViT is problematic from a shift-invariance/smoothness standpoint.</strong> However, there are important caveats that make it <em>less bad</em> than it sounds, plus architectural improvements used in practice.</p>
<hr>
<section id="vanilla-vit-is-not-shift-equivariant" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="vanilla-vit-is-not-shift-equivariant"><span class="header-section-number">5.1</span> Vanilla ViT is Not Shift Equivariant</h3>
<section id="setup" class="level4" data-number="5.1.1">
<h4 data-number="5.1.1" class="anchored" data-anchor-id="setup"><span class="header-section-number">5.1.1</span> Setup</h4>
<ol type="1">
<li>Image ‚Üí split into <strong>non-overlapping</strong> patches (e.g., 16√ó16)</li>
<li>Each patch ‚Üí linear projection ‚Üí token + <strong>absolute positional embedding</strong></li>
<li>Tokens ‚Üí Transformer</li>
</ol>
</section>
<section id="what-happens-with-a-1-pixel-shift" class="level4" data-number="5.1.2">
<h4 data-number="5.1.2" class="anchored" data-anchor-id="what-happens-with-a-1-pixel-shift"><span class="header-section-number">5.1.2</span> What Happens with a 1-Pixel Shift?</h4>
<ul>
<li>Patch boundaries change</li>
<li>Almost <em>all</em> token inputs change</li>
<li>The same visual blob that was at ‚Äúpatch 5‚Äù now lands at ‚Äúpatch 6‚Äù with a different positional embedding</li>
</ul>
<p><strong>So:</strong></p>
<ul>
<li>The mapping is <strong>not</strong> translation equivariant like a CNN</li>
<li>A 1-pixel shift is <em>not</em> guaranteed to correspond to a simple permutation of internal states</li>
<li>The ‚Äúwhole message flow changes‚Äù intuition is conceptually correct</li>
</ul>
<hr>
</section>
</section>
<section id="but-its-not-combinatorially-non-smooth-in-the-math-sense" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="but-its-not-combinatorially-non-smooth-in-the-math-sense"><span class="header-section-number">5.2</span> But it‚Äôs not ‚Äúcombinatorially non-smooth‚Äù in the math sense</h3>
<p>Two separate ideas:</p>
<section id="smoothness-continuity" class="level4" data-number="5.2.1">
<h4 data-number="5.2.1" class="anchored" data-anchor-id="smoothness-continuity"><span class="header-section-number">5.2.1</span> Smoothness (continuity)</h4>
<p>As a function <span class="math display">\[f : \mathbb{R}^{H\times W\times 3} \to \mathbb{R}^d\]</span> (image ‚Üí embedding), a ViT is still:</p>
<ul>
<li><strong>Linear</strong> at the patchification + projection stage.</li>
<li>Followed by attention + MLPs with smooth activations (GELU, etc).</li>
</ul>
<p>So in the strict sense:</p>
<blockquote class="blockquote">
<p>Small change in pixel values ‚áí small change in embeddings (assuming weights are bounded, etc.).</p>
</blockquote>
<p>So it‚Äôs not ‚Äúnon-smooth‚Äù like a hash function; it‚Äôs just <strong>not invariant/equivariant to translations</strong>.</p>
</section>
<section id="why-shifting-by-1-pixel-isnt-as-catastrophic-as-it-sounds" class="level4" data-number="5.2.2">
<h4 data-number="5.2.2" class="anchored" data-anchor-id="why-shifting-by-1-pixel-isnt-as-catastrophic-as-it-sounds"><span class="header-section-number">5.2.2</span> Why shifting by 1 pixel isn‚Äôt <em>as catastrophic</em> as it sounds</h4>
<p>Even with non-overlapping patches:</p>
<ul>
<li>Natural images are <strong>locally redundant</strong>: shifting by 1 px changes patches a lot at the boundary, but content is still very similar in a local neighborhood.</li>
<li>The linear patch projector can learn to be tolerant to small misalignments (just like a 1-layer conv with a big kernel can).</li>
<li>Overparameterization <em>does</em> help: a sufficiently large Transformer can approximate convolutional behavior if the data/augmentations push it in that direction.</li>
</ul>
<p>So in practice, the embedding space <strong>wiggles</strong>, but not in an arbitrarily wild way.</p>
<hr>
</section>
</section>
<section id="how-vits-actually-get-approximate-invariance" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="how-vits-actually-get-approximate-invariance"><span class="header-section-number">5.3</span> How ViTs <em>actually</em> get approximate invariance</h3>
<p>Three big mechanisms:</p>
<section id="data-augmentation" class="level4" data-number="5.3.1">
<h4 data-number="5.3.1" class="anchored" data-anchor-id="data-augmentation"><span class="header-section-number">5.3.1</span> Data augmentation</h4>
<p>Training uses:</p>
<ul>
<li>random crops,</li>
<li>random resize,</li>
<li>random horizontal flips,</li>
<li>sometimes small translations / jitters.</li>
</ul>
<p>If the model must produce the <em>same label</em> for many slightly shifted/cropped versions, gradient descent nudges it toward <strong>approximate shift robustness</strong>, even without a built-in equivariance prior.</p>
<p>So:</p>
<blockquote class="blockquote">
<p>CNNs <em>bake in</em> translation equivariance. ViTs <em>learn</em> approximate invariance from data.</p>
</blockquote>
</section>
<section id="positional-encoding-choices" class="level4" data-number="5.3.2">
<h4 data-number="5.3.2" class="anchored" data-anchor-id="positional-encoding-choices"><span class="header-section-number">5.3.2</span> Positional encoding choices</h4>
<ul>
<li><p><strong>Absolute 2D embeddings</strong> (original ViT) hard-code positions ‚Äî worst for shift invariance.</p></li>
<li><p>Many modern ViTs use:</p>
<ul>
<li><strong>relative positional embeddings</strong>, or</li>
<li><strong>rotary embeddings (RoPE)</strong>, or</li>
<li>learned 2D biases that depend on <em>offsets</em> between tokens.</li>
</ul></li>
</ul>
<p>Those are friendlier to ‚Äúif I move this thing a bit, relationships stay similar.‚Äù</p>
</section>
<section id="convolutional-or-overlapping-stems" class="level4" data-number="5.3.3">
<h4 data-number="5.3.3" class="anchored" data-anchor-id="convolutional-or-overlapping-stems"><span class="header-section-number">5.3.3</span> Convolutional or overlapping ‚Äústems‚Äù</h4>
<p>A lot of ‚ÄúViT‚Äù backbones nowadays are <em>hybrids</em>:</p>
<ul>
<li>A few <strong>conv layers</strong> at the input to produce feature maps, then patchify those.</li>
<li>Or <strong>overlapping patches</strong> (sliding windows) instead of strict disjoint patches.</li>
</ul>
<p>Both soften the ‚Äú1-px shift ‚Üí totally different patches‚Äù issue.</p>
<hr>
</section>
</section>
<section id="multi-resolution-gets-added-back" class="level3" data-number="5.4">
<h3 data-number="5.4" class="anchored" data-anchor-id="multi-resolution-gets-added-back"><span class="header-section-number">5.4</span> Multi-Resolution Gets Added Back</h3>
<p>Multi-resolution helps smoothness and invariance. The ecosystem evolved to include:</p>
<p><strong>Hierarchical architectures</strong> (Swin, PVT, MViT):</p>
<ul>
<li>Patch merging/pooling layers (downsampling)</li>
<li>Multiple feature scales (like CNN pyramids)</li>
<li>Local windows + shifting for gradually expanding receptive fields</li>
</ul>
<p>These are much closer in spirit to multiresolution analysis (though not literally wavelets).</p>
<section id="evolution-timeline" class="level4" data-number="5.4.1">
<h4 data-number="5.4.1" class="anchored" data-anchor-id="evolution-timeline"><span class="header-section-number">5.4.1</span> Evolution Timeline</h4>
<div class="callout callout-style-default callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Original ViT
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Very ‚Äúnaive‚Äù</li>
<li>Single patch scale</li>
<li>Absolute positions</li>
<li>No built-in shift equivariance</li>
<li><strong>Your discomfort is valid</strong> ‚úì</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Practice
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Overparameterization</li>
<li>Massive data</li>
<li>Heavy augmentation</li>
<li>‚Üí Surprisingly robust anyway</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Modern Designs
</div>
</div>
<div class="callout-body-container callout-body">
<p>Add back:</p>
<ul>
<li>Conv stems</li>
<li>Overlapping patches</li>
<li>Multi-scale hierarchies</li>
<li>Relative positions</li>
</ul>
<p>‚Üí Much more well-behaved w.r.t. translations and local distortions</p>
</div>
</div>
<hr>
</section>
</section>
<section id="tldr" class="level3" data-number="5.5">
<h3 data-number="5.5" class="anchored" data-anchor-id="tldr"><span class="header-section-number">5.5</span> TL;DR</h3>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>‚úì <strong>Yes:</strong> Naive ViT has an <em>ugly</em> inductive bias; tiny shifts can rearrange the entire tokenization and attention graph</p>
<p>‚úì <strong>But:</strong> The mapping is still continuous, and with enough data + augmentations + parameters, it <em>learns</em> an approximately smooth, translation-robust embedding</p>
<p>‚úì <strong>In practice:</strong> The community patched the worst issues by adding multi-resolution and conv-ish components, so ‚Äúreal‚Äù ViTs in the wild are much less naive than the toy mental model</p>
</div>
</div>
</div>
<p>üìÖ <em>Last modified: <code>Monday, December 1, 2025</code></em></p>

</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




<script src="2025-12-01-vit-vs-wavelet_files/libs/quarto-html/zenscroll-min.js"></script>
</body></html>