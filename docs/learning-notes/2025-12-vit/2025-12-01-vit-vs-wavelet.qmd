---
title: "ViT vs. Wavelet Intuition"
subtitle: "Patch scale, multiresolution, and shift robustness in ViT backbones"
author: "ChatGPT-5.1 Thinking, Yulin Li"
date: "2025-12-01"
format:
  html:
    toc: true
    toc-depth: 3
    number-sections: true
    code-fold: false
    smooth-scroll: true
    link-external-newwindow: true
    math: mathjax
execute:
  echo: false
  warning: false
  message: false
editor: visual
---

## Background: What is ViT?

::: {.callout-tip icon="false"}
### Question

*What is CLIP in LLM? What is ViT? One-line definitions plz.*
:::

**CLIP (Contrastive Language‚ÄìImage Pre-training)**

:   A vision-language model that jointly trains an image encoder and a text encoder so their embeddings live in the same space, letting models "match" pictures to natural-language descriptions.

**ViT (Vision Transformer)**

:   A vision backbone that treats an image as a sequence of patches and runs a Transformer over them, analogous to how LLMs process token sequences.

------------------------------------------------------------------------

## ViT: Adaptivity to Scale

::: {.callout-tip icon="false"}
### Questions

-   *How does ViT decide the scale to look at?*
-   *How do you know if your visual vocab is expressive enough for the whole space of natural images?*
-   *Usually for image processing we'd need to look at a picture at different scales‚Äîlike Haar wavelets. Was multi-resolution decomposition ever involved in ViT-based architectures?*
:::

This is exactly the right thing to worry about with "image as tokens." Let me break this down:

### How does it decide the scale to look at?

In a plain ViT (like the original Dosovitskiy et al.):

-   You **pick a fixed patch size** (e.g. 16√ó16).
-   The image is split into non-overlapping patches of that size.
-   Each patch ‚Üí linearly projected ‚Üí one token.
-   A Transformer with **global self-attention** lets every patch talk to every other patch.

So:

-   **Local detail** comes from the fact that a token only ‚Äúsees‚Äù its own 16√ó16 pixels before attention.
-   **Global structure** comes from self-attention across *all* tokens, which lets the model integrate info over the full image.

There‚Äôs no explicit ‚Äúpyramid of scales‚Äù like in classical wavelets or CNN feature pyramids. The *scale* is implicitly controlled by:

1.  **Patch size** (how local each token is), and
2.  **Depth of the Transformer** (how many layers of attention you get to mix local‚Üíglobal).

In other words: vanilla ViT has **one native resolution** (fixed patch size), but attention is global, so the *effective* receptive field of each token grows as you go deeper.

### How do you know if your visual vocab is expressive enough?

Here "visual vocab" ‚âà the space of patch embeddings / learned features.

For ViTs:

-   There is **no fixed, pre-defined dictionary** like SIFT/Haar.
-   Instead, the model learns an embedding function `f(patch)` from data, exactly like a word-embedding layer learns meanings from corpus statistics.
-   If the model is trained on a **huge, diverse dataset** (e.g. ImageNet-21k, JFT, LAION) with a strong objective (supervised or contrastive), the learned patch-embedding space tends to become expressive enough in practice.

So how do we *actually* know?

-   Empirically: if a ViT backbone works well across **many downstream tasks** (classification, detection, segmentation, retrieval) with moderate fine-tuning, that‚Äôs evidence your visual ‚Äúvocab‚Äù is rich enough.

-   Theoretically, there‚Äôs no guarantee it perfectly spans ‚Äúthe space of natural images,‚Äù same as we don‚Äôt guarantee LLM token embeddings span ‚Äúall possible meanings.‚Äù It‚Äôs an approximation that improves with:

    -   model capacity (width/depth),
    -   better objectives (contrastive, masked prediction),
    -   and more diverse data.

CLIP is a nice example: it **anchors visual embeddings to language**. If your text side can name "tuxedo cat," "MRI slice," "anime screencap," etc., then the image encoder is pressured to carve up visual space in a way that aligns with those concepts ‚Äî a kind of *semantic* visual vocabulary.

### What about multi-resolution / wavelet-like behavior?

Vanilla ViT:

-   **No explicit multiscale decomposition** like Haar wavelets or image pyramids.
-   It‚Äôs single-scale patches + global attention.

But the ViT *ecosystem* quickly introduced **hierarchical / multiscale variants** precisely because tasks like detection/segmentation *do* benefit from multi-resolution features (like CNNs):

-   **Swin Transformer**: computes self-attention in local windows and **merges patches** between stages ‚Üí a **feature pyramid** (high-res shallow layers, low-res deep layers).
-   **PVT (Pyramid Vision Transformer)**, **MViT (Multiscale ViT)**, etc.: explicitly build **hierarchical, multi-resolution feature maps**, much closer in flavor to FPN/ResNet pyramids.
-   Many modern ViT-based backbones for detection/segmentation give you outputs at multiple strides (e.g., 1/4, 1/8, 1/16 of the input), like a CNN.

So:

-   **Original ViT**: one-scale patches, global attention; multiscale behavior is *implicit* via depth and attention patterns, not a clean wavelet-like decomposition.
-   **Modern ViT-based backbones**: *do* incorporate multi-resolution structures (patch merging, pyramids, windowed attention) so they can act more like classical multiscale vision systems.

### How this ties back to the "LLM analogy"

When I said:

> ‚Äútreats an image as a sequence of patches and runs a Transformer over them, analogous to how LLMs process token sequences‚Äù

more precisely it means:

-   Image patches ‚âà text tokens.
-   Patch embedding layer ‚âà token embedding layer.
-   Self-attention layers operate over **the sequence**, not over a 2D grid with fixed kernels.
-   In vanilla ViT, there‚Äôs **one token granularity** (patch size), just like LLMs don‚Äôt usually change token length at deeper layers.

Multiscale ViT variants correspond, roughly, to **multi-resolution representations** (like CNN pyramids or wavelets), layered on top of this ‚Äúimage-as-sequence‚Äù idea.

------------------------------------------------------------------------

If you‚Äôd like, next step we can:

-   Compare one concrete architecture: plain ViT vs. Swin, and
-   Map it explicitly to wavelet intuition (what in Swin/PVT plays the role of ‚Äúdownsampling‚Äù and ‚Äúlarger-scale‚Äù basis functions).

------------------------------------------------------------------------

## How Attention Grows with Depth

::: {.callout-tip icon="false"}
### Question

*How does attention grow in depth? Do you stack Transformers somehow?*
:::

Yes‚Äîyou stack Transformer blocks, and that's how "mixing over larger scales" happens.

------------------------------------------------------------------------

### Stacking Transformer Blocks

A ViT (and an LLM) has this structure:

``` text
tokens ‚Üí [Block 1] ‚Üí [Block 2] ‚Üí ... ‚Üí [Block L] ‚Üí head
```

Each **Block** consists of:

-   LayerNorm
-   Multi-Head Self-Attention
-   MLP
-   Residual connections

All blocks have the **same structure**, but their weights are different and learned jointly. You're literally stacking self-attention layers, one on top of another.

------------------------------------------------------------------------

### How does that make attention "grow"?

Two cases: **global attention** (plain ViT) vs **local/windowed attention** (e.g., Swin).

#### Vanilla ViT (global self-attention)

-   In *one* layer, each token can already attend to **all** other tokens.
-   So in a strict graph-theory sense, the receptive field is already ‚Äúglobal‚Äù in one hop.

So why does depth still matter?

-   Early layers tend to encode **local, low-level stuff** (edges, textures within a patch and near neighbors).

-   Later layers operate on **already-processed features**, so they can:

    -   attend to *summaries* (e.g., ‚Äúthis patch already encodes ‚Äòcat ear‚Äô or ‚Äòcolumn of text‚Äô‚Äù),
    -   combine those into more global concepts (‚Äúwhole cat,‚Äù ‚Äúentire title line‚Äù),
    -   refine and reweight interactions iteratively (like multiple rounds of message passing).

So depth ‚âà *more rounds of ‚Äúwho should I pay attention to, given what I‚Äôve learned so far?‚Äù*. Even though every layer can see the whole sequence, what they *do* with that view becomes more global and abstract as you go deeper.

------------------------------------------------------------------------

#### Windowed/Local Attention (Swin, etc.)

Here it's more literally "growing receptive field":

1.  **Layer 1**: Each token attends only within a small window (e.g., 7√ó7 patches)
2.  **Layer 2**: Windows shift / patches merge ‚Üí each token now indirectly depends on a **bigger neighborhood**
3.  **After several layers**: The "communication radius" expands; tokens end up influenced by faraway regions

This is the Transformer analogue of: **more conv layers ‚Üí larger effective receptive field**.

------------------------------------------------------------------------

### Core Intuition

::: {.callout-note appearance="minimal" icon="false"}
We stack many Transformer blocks; each self-attention layer lets tokens re-mix information based on what they've already computed. As you go deeper, tokens' representations fold in more context and more abstract structure‚Äîeven if each layer is "just" the same attention mechanism repeated.
:::

------------------------------------------------------------------------

## ViT: Recognition and Generation

::: {.callout-tip icon="false"}
### Question

*Is ViT used only for recognition? Or generation as well?*
:::

Definitely **not** only for recognition‚ÄîViT is used for **both**.

### Recognition/Perception

The original ViT was for image classification. ViT-style backbones are now everywhere:

-   Image classification
-   Object detection
-   Semantic segmentation
-   Image retrieval
-   CLIP-style encoders

### Generation

ViTs are used in two ways:

**As encoders feeding generators**

:   Examples: ViT ‚Üí LLM ‚Üí text, or ViT ‚Üí diffusion model ‚Üí edited image

**As image generators themselves**

:   When you treat image patches as tokens and train a Transformer to *predict or denoise* them:

-   Masked image modeling
-   Autoregressive image token models
-   Diffusion Transformers ("DiT"-style models)

::: callout-note
**Key takeaway:** ViT = a general-purpose vision backbone. Whether it "recognizes" or "generates" depends on the head + training objective, not the architecture itself.
:::

------------------------------------------------------------------------

## ViT: Efficiency of Representation üîç

::: callout-important
## Question

*The naive ViT idea seems very weird. Without multi-resolution adaptability, the function from image to visual tokens/image embedding looks very non-smooth. If you shift the image by 1 column of pixels to the right, then the whole message flow changes‚Äîunless you heavily overparametrize...?*
:::

This intuition is on point: **a strictly "naive" ViT is problematic from a shift-invariance/smoothness standpoint.** However, there are important caveats that make it *less bad* than it sounds, plus architectural improvements used in practice.

------------------------------------------------------------------------

### Vanilla ViT is Not Shift Equivariant

#### Setup

1.  Image ‚Üí split into **non-overlapping** patches (e.g., 16√ó16)
2.  Each patch ‚Üí linear projection ‚Üí token + **absolute positional embedding**
3.  Tokens ‚Üí Transformer

#### What Happens with a 1-Pixel Shift?

-   Patch boundaries change
-   Almost *all* token inputs change
-   The same visual blob that was at "patch 5" now lands at "patch 6" with a different positional embedding

**So:**

-   The mapping is **not** translation equivariant like a CNN
-   A 1-pixel shift is *not* guaranteed to correspond to a simple permutation of internal states
-   The "whole message flow changes" intuition is conceptually correct

------------------------------------------------------------------------

### But it's not "combinatorially non-smooth" in the math sense

Two separate ideas:

#### Smoothness (continuity)

As a function $$f : \mathbb{R}^{H\times W\times 3} \to \mathbb{R}^d$$ (image ‚Üí embedding), a ViT is still:

-   **Linear** at the patchification + projection stage.
-   Followed by attention + MLPs with smooth activations (GELU, etc).

So in the strict sense:

> Small change in pixel values ‚áí small change in embeddings (assuming weights are bounded, etc.).

So it's not "non-smooth" like a hash function; it's just **not invariant/equivariant to translations**.

#### Why shifting by 1 pixel isn't *as catastrophic* as it sounds

Even with non-overlapping patches:

-   Natural images are **locally redundant**: shifting by 1 px changes patches a lot at the boundary, but content is still very similar in a local neighborhood.
-   The linear patch projector can learn to be tolerant to small misalignments (just like a 1-layer conv with a big kernel can).
-   Overparameterization *does* help: a sufficiently large Transformer can approximate convolutional behavior if the data/augmentations push it in that direction.

So in practice, the embedding space **wiggles**, but not in an arbitrarily wild way.

------------------------------------------------------------------------

### How ViTs *actually* get approximate invariance

Three big mechanisms:

#### Data augmentation

Training uses:

-   random crops,
-   random resize,
-   random horizontal flips,
-   sometimes small translations / jitters.

If the model must produce the *same label* for many slightly shifted/cropped versions, gradient descent nudges it toward **approximate shift robustness**, even without a built-in equivariance prior.

So:

> CNNs *bake in* translation equivariance. ViTs *learn* approximate invariance from data.

#### Positional encoding choices

-   **Absolute 2D embeddings** (original ViT) hard-code positions ‚Äî worst for shift invariance.

-   Many modern ViTs use:

    -   **relative positional embeddings**, or
    -   **rotary embeddings (RoPE)**, or
    -   learned 2D biases that depend on *offsets* between tokens.

Those are friendlier to "if I move this thing a bit, relationships stay similar."

#### Convolutional or overlapping "stems"

A lot of "ViT" backbones nowadays are *hybrids*:

-   A few **conv layers** at the input to produce feature maps, then patchify those.
-   Or **overlapping patches** (sliding windows) instead of strict disjoint patches.

Both soften the ‚Äú1-px shift ‚Üí totally different patches‚Äù issue.

------------------------------------------------------------------------

### Multi-Resolution Gets Added Back

Multi-resolution helps smoothness and invariance. The ecosystem evolved to include:

**Hierarchical architectures** (Swin, PVT, MViT):

-   Patch merging/pooling layers (downsampling)
-   Multiple feature scales (like CNN pyramids)
-   Local windows + shifting for gradually expanding receptive fields

These are much closer in spirit to multiresolution analysis (though not literally wavelets).

#### Evolution Timeline

::: {.callout-caution icon="false"}
##### Original ViT

-   Very "naive"
-   Single patch scale
-   Absolute positions
-   No built-in shift equivariance
-   **Your discomfort is valid** ‚úì
:::

::: {.callout-warning icon="false"}
##### Practice

-   Overparameterization
-   Massive data
-   Heavy augmentation
-   ‚Üí Surprisingly robust anyway
:::

::: {.callout-tip icon="false"}
##### Modern Designs

Add back:

-   Conv stems
-   Overlapping patches
-   Multi-scale hierarchies
-   Relative positions

‚Üí Much more well-behaved w.r.t. translations and local distortions
:::

------------------------------------------------------------------------

### TL;DR

::: {.callout-note appearance="minimal"}
‚úì **Yes:** Naive ViT has an *ugly* inductive bias; tiny shifts can rearrange the entire tokenization and attention graph

‚úì **But:** The mapping is still continuous, and with enough data + augmentations + parameters, it *learns* an approximately smooth, translation-robust embedding

‚úì **In practice:** The community patched the worst issues by adding multi-resolution and conv-ish components, so "real" ViTs in the wild are much less naive than the toy mental model
:::

üìÖ *Last modified: `Monday, December 1, 2025`*